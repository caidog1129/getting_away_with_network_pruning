@article{local-competition,
 author = {Panousis, Konstantinos P and Chatzis, Sotirios and Theodoridis, Sergios},
 journal = {arXiv preprint arXiv:1805.07624},
 title = {Nonparametric Bayesian Deep Networks with Local Competition},
 year = {2018}
}

@inproceedings{peng-collaborative,
 author = {Peng, Hanyu and Wu, Jiaxiang and Chen, Shifeng and Huang, Junzhou},
 booktitle = {International Conference on Machine Learning},
 pages = {5113--5122},
 title = {Collaborative Channel Pruning for Deep Networks},
 year = {2019}
}

@article{eigenDamage,
 author = {Wang, Chaoqi and Grosse, Roger and Fidler, Sanja and Zhang, Guodong},
 journal = {arXiv preprint arXiv:1905.05934},
 title = {EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis},
 year = {2019}
}

@article{SNIP,
 author = {Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS},
 journal = {arXiv preprint arXiv:1810.02340},
 title = {SNIP: Single-shot network pruning based on connection sensitivity},
 year = {2018}
}

@article{samsung-differentiable,
 author = {Kim, Jaedeok and Park, Chiyoun and Jung, Hyun-Joo and Choe, Yoonsuck},
 journal = {arXiv preprint arXiv:1904.10921},
 title = {Differentiable Pruning Method for Neural Networks},
 year = {2019}
}

@article{L0-arm-binary,
 author = {Li, Yang and Ji, Shihao},
 journal = {arXiv preprint arXiv:1904.04432},
 title = {$ L\_0 $-ARM: Network Sparsification via Stochastic Binary Optimization},
 year = {2019}
}

@article{lottery-ticket-followup,
 author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M and Carbin, Michael},
 journal = {arXiv preprint arXiv:1903.01611},
 title = {The Lottery Ticket Hypothesis at Scale},
 year = {2019}
}

@article{rethinking-net-pruning,
 author = {Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
 journal = {arXiv preprint arXiv:1810.05270},
 title = {Rethinking the value of network pruning},
 year = {2018}
}

@article{net-trim-v2,
 author = {Aghasi, Alireza and Abdi, Afshin and Romberg, Justin},
 journal = {arXiv preprint arXiv:1806.06457},
 title = {Fast Convex Pruning of Deep Neural Networks},
 year = {2018}
}

@article{mit-coreset-pruning,
 author = {Baykal, Cenk and Liebenwein, Lucas and Gilitschenski, Igor and Feldman, Dan and Rus, Daniela},
 journal = {arXiv preprint arXiv:1804.05345},
 title = {Data-dependent coresets for compressing neural networks with applications to generalization bounds},
 year = {2018}
}

@article{samsung-winograd-sparse,
 author = {Choi, Yoojin and El-Khamy, Mostafa and Lee, Jungwon},
 journal = {arXiv preprint arXiv:1902.08192},
 title = {Jointly Sparse Convolutional Neural Networks in Dual Spatial-Winograd Domains},
 year = {2019}
}

@misc{google-state-of-sparsity,
 archiveprefix = {arXiv},
 author = {Trevor Gale and Erich Elsen and Sara Hooker},
 eprint = {1902.09574},
 primaryclass = {cs.LG},
 title = {The State of Sparsity in Deep Neural Networks},
 year = {2019}
}

@article{autopruner,
 author = {Luo, Jian-Hao and Wu, Jianxin},
 journal = {arXiv preprint arXiv:1805.08941},
 title = {Autopruner: An end-to-end trainable filter pruning method for efficient deep model inference},
 year = {2018}
}

@inproceedings{synaptic-strength,
 author = {Lin, Chen and Zhong, Zhao and Wei, Wu and Yan, Junjie},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {10149--10158},
 title = {Synaptic Strength For Convolutional Neural Network},
 year = {2018}
}

@article{pcas,
 author = {Yamamoto, Kohei and Maeno, Kurato},
 journal = {arXiv preprint arXiv:1806.05382},
 title = {Pcas: Pruning channels with attention statistics},
 year = {2018}
}

@article{balanced-sparsity,
 author = {Yao, Zhuliang and Cao, Shijie and Xiao, Wencong},
 journal = {arXiv preprint arXiv:1811.00206},
 title = {Balanced Sparsity for Efficient DNN Inference on GPU},
 year = {2018}
}

@inproceedings{zhuang-discriminative-channel,
 author = {Zhuang, Zhuangwei and Tan, Mingkui and Zhuang, Bohan and Liu, Jing and Guo, Yong and Wu, Qingyao and Huang, Junzhou and Zhu, Jinhui},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {875--886},
 title = {Discrimination-aware channel pruning for deep neural networks},
 year = {2018}
}

@article{lottery-ticket,
 author = {Frankle, Jonathan and Carbin, Michael},
 journal = {arXiv preprint arXiv:1803.03635},
 title = {The lottery ticket hypothesis: Finding sparse, trainable neural networks},
 year = {2018}
}

@article{crossbar-aware,
 author = {Liang, Ling and Deng, Lei and Zeng, Yueling and Hu, Xing and Ji, Yu and Ma, Xin and Li, Guoqi and Xie, Yuan},
 journal = {IEEE Access},
 pages = {58324--58337},
 publisher = {IEEE},
 title = {Crossbar-aware neural network pruning},
 volume = {6},
 year = {2018}
}

@inproceedings{uiuc-coreset-pruning,
 author = {Dubey, Abhimanyu and Chatterjee, Moitreya and Ahuja, Narendra},
 booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
 pages = {454--470},
 title = {Coreset-based neural network compression},
 year = {2018}
}

@inproceedings{sss,
 author = {Huang, Zehao and Wang, Naiyan},
 booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
 pages = {304--320},
 title = {Data-driven sparse structure selection for deep neural networks},
 year = {2018}
}

@article{sparse-evolutionary,
 author = {Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
 journal = {Nature communications},
 number = {1},
 pages = {2383},
 publisher = {Nature Publishing Group},
 title = {Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
 volume = {9},
 year = {2018}
}

@inproceedings{extreme-net-compress,
 author = {Peng, Bo and Tan, Wenming and Li, Zheyang and Zhang, Shun and Xie, Di and Pu, Shiliang},
 booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
 pages = {300--316},
 title = {Extreme network compression via filter group approximation},
 year = {2018}
}

@article{apple-pfa,
 author = {Suau, Xavier and Zappella, Luca and Apostoloff, Nicholas},
 title = {NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES},
 year = {2018}
}

@inproceedings{soft-filter-pruning,
 author = {He, Y and Kang, G and Dong, X and Fu, Y and Yang, Y},
 booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
 title = {Soft filter pruning for accelerating deep convolutional neural networks},
 year = {2018}
}

@inproceedings{amc-automl-han,
 author = {He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
 booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
 pages = {784--800},
 title = {Amc: Automl for model compression and acceleration on mobile devices},
 year = {2018}
}

@article{spectral-pruning,
 author = {Suzuki, Taiji and Abe, Hiroshi and Murata, Tomoya and Horiuchi, Shingo and Ito, Kotaro and Wachi, Tokuma and Hirai, So and Yukishima, Masatoshi and Nishimura, Tomoaki},
 journal = {arXiv preprint arXiv:1808.08558},
 title = {Spectral-Pruning: Compressing deep neural network via spectral analysis},
 year = {2018}
}

@inproceedings{learning-compression,
 author = {Carreira-Perpin{\'a}n, Miguel A and Idelbayev, Yerlan},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
 pages = {8532--8541},
 title = {“Learning-Compression” Algorithms for Neural Net Pruning},
 year = {2018}
}

@article{smallify,
 author = {Leclerc, Guillaume and Vartak, Manasi and Fernandez, Raul Castro and Kraska, Tim and Madden, Samuel},
 journal = {arXiv preprint arXiv:1806.03723},
 title = {Smallify: Learning Network Size while Training},
 year = {2018}
}

@inproceedings{nisp,
 author = {Yu, Ruichi and Li, Ang and Chen, Chun-Fu and Lai, Jui-Hsin and Morariu, Vlad I and Han, Xintong and Gao, Mingfei and Lin, Ching-Yung and Davis, Larry S},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
 pages = {9194--9203},
 title = {Nisp: Pruning networks using neuron importance score propagation},
 year = {2018}
}

@article{dai-info-bottleneck,
 author = {Dai, Bin and Zhu, Chen and Wipf, David},
 journal = {arXiv preprint arXiv:1802.10399},
 title = {Compressing neural networks using the variational information bottleneck},
 year = {2018}
}

@inproceedings{huang-prune-filters,
 author = {Huang, Qiangui and Zhou, Kevin and You, Suya and Neumann, Ulrich},
 booktitle = {2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
 organization = {IEEE},
 pages = {709--718},
 title = {Learning to prune filters in convolutional neural networks},
 year = {2018}
}

@inproceedings{ding-auto-balanced,
 author = {Ding, Xiaohan and Ding, Guiguang and Han, Jungong and Tang, Sheng},
 booktitle = {Thirty-Second AAAI Conference on Artificial Intelligence},
 title = {Auto-balanced filter pruning for efficient convolutional neural networks},
 year = {2018}
}

@article{rethinking-smaller-norm,
 author = {Ye, Jianbo and Lu, Xin and Lin, Zhe and Wang, James Z},
 journal = {arXiv preprint arXiv:1802.00124},
 title = {Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers},
 year = {2018}
}

@article{lstm-group-lasso,
 author = {Wen, Wei and He, Yuxiong and Rajbhandari, Samyam and Zhang, Minjia and Wang, Wenhan and Liu, Fang and Hu, Bin and Chen, Yiran and Li, Hai},
 journal = {arXiv preprint arXiv:1709.05027},
 title = {Learning intrinsic sparse structures within long short-term memory},
 year = {2017}
}

@inproceedings{net-trim,
 author = {Aghasi, Alireza and Abdi, Afshin and Nguyen, Nam and Romberg, Justin},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {3177--3186},
 title = {Net-trim: Convex pruning of deep neural networks with performance guarantee},
 year = {2017}
}

@inproceedings{compression-aware-training,
 author = {Alvarez, Jose M and Salzmann, Mathieu},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {856--867},
 title = {Compression-aware training of deep networks},
 year = {2017}
}

@article{openai-block-sparse,
 author = {Gray, Scott and Radford, Alec and Kingma, Diederik P},
 journal = {arXiv preprint arXiv:1711.09224},
 title = {Gpu kernels for block-sparse weights},
 year = {2017}
}

@inproceedings{runtime-neural-pruning,
 author = {Lin, Ji and Rao, Yongming and Lu, Jiwen and Zhou, Jie},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {2181--2191},
 title = {Runtime neural pruning},
 year = {2017}
}

@article{hard-concrete,
 author = {Louizos, Christos and Welling, Max and Kingma, Diederik P},
 journal = {arXiv preprint arXiv:1712.01312},
 title = {Learning Sparse Neural Networks through $ L\_0 $ Regularization},
 year = {2017}
}

@inproceedings{bayesian-compression,
 author = {Louizos, Christos and Ullrich, Karen and Welling, Max},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {3288--3298},
 title = {Bayesian compression for deep learning},
 year = {2017}
}

@inproceedings{structured-log-normal,
 author = {Neklyudov, Kirill and Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry P},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {6775--6784},
 title = {Structured bayesian pruning via log-normal multiplicative noise},
 year = {2017}
}

@article{block-sparse-rnns,
 author = {Narang, Sharan and Undersander, Eric and Diamos, Gregory},
 journal = {arXiv preprint arXiv:1711.02782},
 title = {Block-sparse recurrent neural networks},
 year = {2017}
}

@inproceedings{channel-lasso-lstsq,
 author = {He, Yihui and Zhang, Xiangyu and Sun, Jian},
 booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
 pages = {1389--1397},
 title = {Channel pruning for accelerating very deep neural networks},
 year = {2017}
}

@inproceedings{network-slimming,
 author = {Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
 booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
 pages = {2736--2744},
 title = {Learning efficient convolutional networks through network slimming},
 year = {2017}
}

@inproceedings{more-is-less,
 author = {Dong, Xuanyi and Huang, Junshi and Yang, Yi and Yan, Shuicheng},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
 pages = {5840--5848},
 title = {More is less: A more complicated network with less inference complexity},
 year = {2017}
}

@inproceedings{thinet-channel-norms,
 author = {Luo, Jian-Hao and Wu, Jianxin and Lin, Weiyao},
 booktitle = {Proceedings of the IEEE international conference on computer vision},
 pages = {5058--5066},
 title = {Thinet: A filter level pruning method for deep neural network compression},
 year = {2017}
}

@inproceedings{sze-energy-aware,
 author = {Yang, Tien-Ju and Chen, Yu-Hsin and Sze, Vivienne},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
 pages = {5687--5695},
 title = {Designing energy-efficient convolutional neural networks using energy-aware pruning},
 year = {2017}
}

@inproceedings{sparse-variational-dropout,
 author = {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
 booktitle = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
 organization = {JMLR. org},
 pages = {2498--2507},
 title = {Variational dropout sparsifies deep neural networks},
 year = {2017}
}

@inproceedings{babu-training-sparse,
 author = {Srinivas, Suraj and Subramanya, Akshayvarun and Venkatesh Babu, R},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
 pages = {138--145},
 title = {Training sparse neural networks},
 year = {2017}
}

@article{nvidia-taylor-pruning,
 author = {Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
 journal = {arXiv preprint arXiv:1611.06440},
 title = {Pruning convolutional neural networks for resource efficient inference},
 year = {2016}
}

@article{pruning-filters,
 author = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
 journal = {arXiv preprint arXiv:1608.08710},
 title = {Pruning filters for efficient convnets},
 year = {2016}
}

@article{google-interchannel,
 author = {Changpinyo, Soravit and Sandler, Mark and Zhmoginov, Andrey},
 journal = {arXiv preprint arXiv:1702.06257},
 title = {The power of sparsity in convolutional neural networks},
 year = {2017}
}

@article{welling-slow-quantize+prune,
 author = {Ullrich, Karen and Meeds, Edward and Welling, Max},
 journal = {arXiv preprint arXiv:1702.04008},
 title = {Soft weight-sharing for neural network compression},
 year = {2017}
}

@inproceedings{learning-num-neurons,
 author = {Alvarez, Jose M and Salzmann, Mathieu},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {2270--2278},
 title = {Learning the number of neurons in deep networks},
 year = {2016}
}

@inproceedings{perforated-cnns,
 author = {Figurnov, Mikhail and Ibraimova, Aizhan and Vetrov, Dmitry P and Kohli, Pushmeet},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {947--955},
 title = {Perforatedcnns: Acceleration through elimination of redundant convolutions},
 year = {2016}
}

@inproceedings{net-surgery,
 author = {Guo, Yiwen and Yao, Anbang and Chen, Yurong},
 booktitle = {Advances In Neural Information Processing Systems},
 pages = {1379--1387},
 title = {Dynamic network surgery for efficient dnns},
 year = {2016}
}

@inproceedings{ssl,
 author = {Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
 booktitle = {Advances in neural information processing systems},
 pages = {2074--2082},
 title = {Learning structured sparsity in deep neural networks},
 year = {2016}
}

@article{babu-generalized-dropout,
 author = {Srinivas, Suraj and Babu, R Venkatesh},
 journal = {arXiv preprint arXiv:1611.06791},
 title = {Generalized dropout},
 year = {2016}
}

@article{course-pruning,
 author = {Anwar, Sajid and Sung, Wonyong},
 journal = {arXiv preprint arXiv:1610.09639},
 title = {Compact deep convolutional neural networks with coarse pruning},
 year = {2016}
}

@article{babu-learning-architecture,
 author = {Srinivas, Suraj and Babu, R Venkatesh},
 journal = {arXiv preprint arXiv:1511.05497},
 title = {Learning neural network architectures using backpropagation},
 year = {2015}
}

@article{net-trimming-apoz,
 author = {Hu, Hengyuan and Peng, Rui and Tai, Yu-Wing and Tang, Chi-Keung},
 journal = {arXiv preprint arXiv:1607.03250},
 title = {Network trimming: A data-driven neuron pruning approach towards efficient deep architectures},
 year = {2016}
}

@article{pan-dropneuron,
 author = {Pan, Wei and Dong, Hao and Guo, Yike},
 journal = {arXiv preprint arXiv:1606.07326},
 title = {Dropneuron: Simplifying the structure of deep neural networks},
 year = {2016}
}

@article{group-sparse-dnns,
 author = {Scardapane, Simone and Comminiello, Danilo and Hussain, Amir and Uncini, Aurelio},
 journal = {Neurocomputing},
 pages = {81--89},
 publisher = {Elsevier},
 title = {Group sparse regularization for deep neural networks},
 volume = {241},
 year = {2017}
}

@inproceedings{lempitsky-fast-convnets,
 author = {Lebedev, Vadim and Lempitsky, Victor},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
 pages = {2554--2564},
 title = {Fast convnets using group-wise brain damage},
 year = {2016}
}

@article{samsung-vbmf-tucker,
 author = {Kim, Yong-Deok and Park, Eunhyeok and Yoo, Sungjoo and Choi, Taelim and Yang, Lu and Shin, Dongjun},
 journal = {arXiv preprint arXiv:1511.06530},
 title = {Compression of deep convolutional neural networks for fast and low power mobile applications},
 year = {2015}
}

@article{divnet,
 author = {Mariet, Zelda and Sra, Suvrit},
 journal = {arXiv preprint arXiv:1511.05077},
 title = {Diversity networks: Neural network compression using determinantal point processes},
 year = {2015}
}

@article{han-prune-quant-huff,
 author = {Han, Song and Mao, Huizi and Dally, William J},
 journal = {arXiv preprint arXiv:1510.00149},
 title = {Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
 year = {2015}
}

@inproceedings{learning-both,
 author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
 booktitle = {Advances in neural information processing systems},
 pages = {1135--1143},
 title = {Learning both weights and connections for efficient neural network},
 year = {2015}
}

@article{zhang-accel-very-deep,
 author = {Zhang, Xiangyu and Zou, Jianhua and He, Kaiming and Sun, Jian},
 journal = {IEEE transactions on pattern analysis and machine intelligence},
 number = {10},
 pages = {1943--1955},
 publisher = {IEEE},
 title = {Accelerating very deep convolutional networks for classification and detection},
 volume = {38},
 year = {2015}
}

@article{face-prune,
 author = {Polyak, Adam and Wolf, Lior},
 journal = {IEEE Access},
 pages = {2163--2175},
 publisher = {IEEE},
 title = {Channel-level acceleration of deep face representations},
 volume = {3},
 year = {2015}
}

@article{babu-data-free-pruning,
 author = {Srinivas, Suraj and Babu, R Venkatesh},
 journal = {arXiv preprint arXiv:1507.06149},
 title = {Data-free parameter pruning for deep neural networks},
 year = {2015}
}

@inproceedings{liu-sparse-conv,
 author = {Liu, Baoyuan and Wang, Min and Foroosh, Hassan and Tappen, Marshall and Pensky, Marianna},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
 pages = {806--814},
 title = {Sparse convolutional neural networks},
 year = {2015}
}

@article{lempitsky-cp-decomp,
 author = {Lebedev, Vadim and Ganin, Yaroslav and Rakhuba, Maksim and Oseledets, Ivan and Lempitsky, Victor},
 journal = {arXiv preprint arXiv:1412.6553},
 title = {Speeding-up convolutional neural networks using fine-tuned cp-decomposition},
 year = {2014}
}

@article{memory-bounded-convnets,
 author = {Collins, Maxwell D and Kohli, Pushmeet},
 journal = {arXiv preprint arXiv:1412.1442},
 title = {Memory bounded deep convolutional networks},
 year = {2014}
}

@inproceedings{exploit-linear-structure,
 author = {Denton, Emily L and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
 booktitle = {Advances in neural information processing systems},
 pages = {1269--1277},
 title = {Exploiting linear structure within convolutional networks for efficient evaluation},
 year = {2014}
}

@inproceedings{he-reshaping,
 author = {He, Tianxing and Fan, Yuchen and Qian, Yanmin and Tan, Tian and Yu, Kai},
 booktitle = {2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
 organization = {IEEE},
 pages = {245--249},
 title = {Reshaping deep neural network for fast decoding by node-pruning},
 year = {2014}
}

@inproceedings{jaderberg-low-rank-conv,
 author = {Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
 booktitle = {Proceedings of the British Machine Vision Conference. BMVA Press},
 title = {Speeding up Convolutional Neural Networks with Low Rank Expansions},
 year = {2014}
}

@inproceedings{early-brain-damage,
 author = {Tresp, Volker and Neuneier, Ralph and Zimmermann, Hans-Georg},
 booktitle = {Advances in neural information processing systems},
 pages = {669--675},
 title = {Early brain damage},
 year = {1997}
}

@inproceedings{optimal-brain-surgeon,
 author = {Hassibi, Babak and Stork, David G and Wolff, Gregory J},
 booktitle = {IEEE international conference on neural networks},
 organization = {IEEE},
 pages = {293--299},
 title = {Optimal brain surgeon and general network pruning},
 year = {1993}
}

@inproceedings{optimal-brain-damage,
 author = {LeCun, Yann and Denker, John S and Solla, Sara A},
 booktitle = {Advances in neural information processing systems},
 pages = {598--605},
 title = {Optimal brain damage},
 year = {1990}
}

